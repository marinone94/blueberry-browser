# AI Proactive Task Intelligence Implementation

## Overview

**AI Proactive Task Intelligence** is an advanced feature that analyzes user browsing behavior to detect patterns and provide actionable, personalized insights. Unlike traditional browsers that wait for user input, this feature proactively identifies workflows, research topics, abandoned tasks, and habits to enhance productivity.

## Key Innovation

### Why This Is Superior

**Competitors (Strawberry)** are **reactive**: You tell them what to do, they execute.

**Blueberry is proactive**: It watches, learns, and suggests WITHOUT being asked. This represents the future of intelligent browsing.

### Feature Highlights

1. **Pure Semantic Session Segmentation** - Uses LLM to understand context switches (no arbitrary time limits)
2. **Multi-Strategy Pattern Detection** - Finds 4 types of patterns simultaneously
3. **One-Click Actions** - Execute workflows or resume tasks instantly
4. **Fully Leverages Existing Infrastructure** - Uses content analysis, vector search, and activity tracking already built
5. **Foundation for Agentic AI workflows** - Uses existing infrastructure to build agentic workflows

## Algorithm Design

### 1. Data Flow

```
[Raw Activity Logs (7 days)] 
    â†“
[Content Analysis Data] (categories, descriptions, screenshots)
    â†“
[LLM Session Segmentation] (semantic context switches)
    â†“
[Pattern Detection] (Parallel: Sequential, Topic, Abandonment, Temporal)
    â†“
[Scoring & Ranking] (frequency Ã— recency Ã— impact)
    â†“
[Actionable Insights] (title, description, one-click action)
```

### 2. Pure Semantic Session Segmentation

**Problem**: Traditional approaches use arbitrary time limits (e.g., 30min gaps) that don't reflect actual user context.

**Solution**: Use GPT-5-nano to analyze full content context and decide if two consecutive pages represent a semantic context switch. No time limits - sessions can be 5 minutes or 2 hours based on actual behavior.

**Input to LLM**:
- URL, title, category/subcategory, brand
- Page description (first 200 chars)
- Screenshot description (first 200 chars)
- Primary language

**Output**: `{ decision: "NEW" | "SAME", reason: "...", confidence: 0.0-1.0 }`

**Why this works**:
- Captures brand exploration (browsing Intercom docs = one session)
- Detects topic continuity (comparing customer service tools = one session)
- Language-aware (switching languages often means context switch)
- Fast & cheap (~30ms, $0.00003 per decision)

### 3. Multi-Strategy Pattern Detection

#### A. Sequential Patterns (Workflows)

**Goal**: Detect recurring tab sequences (e.g., Gmail â†’ Calendar â†’ Slack every morning).

**Algorithm**:
1. Extract content sequences: `[(category, subcategory, brand), ...]` for each session
2. Compare all session pairs for similarity (category 40%, subcategory 30%, brand 30%)
3. Group similar sequences (similarity > 0.7)
4. Filter by frequency (â‰¥2 occurrences)
5. LLM names the workflow (e.g., "Daily productivity start")

**Why not just URLs?**
- URLs change, but semantic patterns persist
- "Intercom pricing" + "Zendesk pricing" = "Comparing customer service tools"

#### B. Topic Patterns (Research Sessions)

**Goal**: Identify recurring research topics and extract key learnings.

**Algorithm**:
1. Group sessions by primary category
2. For categories with â‰¥2 sessions, collect all page descriptions
3. LLM analyzes: "What is the user trying to learn?"
4. LLM extracts key insights from descriptions

**Output**: Research summary with actionable next steps.

#### C. Abandonment Patterns (Unfinished Tasks)

**Goal**: Detect tasks user started but didn't complete.

**Algorithm**:
1. For each session, LLM analyzes the timeline:
   - What was the intent?
   - What progress was made?
   - Was it completed or abandoned? Why?
   - Completion score (0.0-1.0)
2. Filter sessions with completion score < 0.6
3. LLM suggests 2-3 ways to resume

**Why LLM is essential here**:
- Heuristics fail (time-on-page means nothing without context)
- LLM understands intent ("researching pricing" vs "random browsing")

#### D. Temporal Patterns (Habits)

**Goal**: Detect time-based browsing habits.

**Algorithm**:
1. Group activities by `(day_of_week, hour, domain)`
2. Find patterns with frequency â‰¥3
3. Simple confidence calculation

**Example**: "You usually visit gmail.com on Monday at 9:00"

### 4. Scoring & Ranking

**Composite Score**:
```
score = frequency Ã— 0.3 + recency Ã— 0.3 + impact Ã— 0.4
```

**Where**:
- **Frequency**: How often pattern occurs (normalized to 0-1)
- **Recency**: Exponential decay based on days since last occurrence
- **Impact**: Estimated time saved if automated (or time wasted if abandoned)

**Why this works**: Multi-dimensional ranking ensures most valuable patterns surface.

### 5. Cost Analysis

**Per user, per day** (assuming 8h browsing, 50 activities, 5 sessions):

| Operation | Model | Tokens | Count | Daily Cost |
|-----------|-------|--------|-------|------------|
| Session boundary | gpt-5-nano | 200 | 50 | $0.0015 |
| Pattern themes | gpt-5-nano | 300 | 3 | $0.00015 |
| Topic analysis | gpt-5-nano | 500 | 2 | $0.0026 |
| Completion analysis | gpt-5-nano | 500 | 5 | $0.0065 |
| **TOTAL** | | | | **~$0.014/day** |

**Result**: **$5/year per active user** - entirely feasible.

## Implementation Details

### Architecture

```
ProactiveInsightsManager (Main Process)
    â†“ IPC
EventManager (IPC Handlers)
    â†“ IPC
Sidebar Preload (API Bridge)
    â†“
InsightsContext (React Context)
    â†“
Insights Component (UI)
```

### Key Files

**Backend**:
- `src/main/ProactiveInsightsManager.ts` - Core pattern detection logic (1200 lines)
- `src/main/EventManager.ts` - IPC handlers for insights
- `src/main/Window.ts` - Manager initialization

**Preload**:
- `src/preload/sidebar.ts` - IPC API exposure
- `src/preload/sidebar.d.ts` - TypeScript definitions

**Frontend**:
- `src/renderer/sidebar/src/contexts/InsightsContext.tsx` - State management
- `src/renderer/sidebar/src/components/Insights.tsx` - UI component
- `src/renderer/sidebar/src/SidebarApp.tsx` - Integration

### IPC API

```typescript
// Analyze behavior (runs pattern detection)
window.sidebarAPI.analyzeBehavior() â†’ Promise<ProactiveInsight[]>

// Get cached insights
window.sidebarAPI.getInsights() â†’ Promise<ProactiveInsight[]>

// Execute insight action
window.sidebarAPI.executeInsightAction(insightId) â†’ Promise<{success, message}>

// Check real-time triggers (future use)
window.sidebarAPI.checkInsightTriggers(url, activities) â†’ Promise<ProactiveInsight[]>
```

### Data Sources

The feature leverages existing data:
1. **Raw Activity Logs** (`/users/user-data/{userId}/raw-activity/*.json`) - Last 7 days
2. **Content Analysis** (`/users/user-data/{userId}/content-analysis/*.json`) - Rich metadata
3. **Vector Embeddings** (future use for semantic similarity)

## User Experience

### Flow

1. User browses normally (data collected passively)
2. User clicks "Insights" button in sidebar
3. Click "Analyze Behavior" (takes ~10-30 seconds)
4. See personalized insights:
   - **Workflow**: "Daily news catchup" - Open 3 tabs
   - **Research**: "Researching AI customer service tools" - Key findings
   - **Abandoned**: "Product comparison" - Resume where you left off
   - **Habit**: "You usually visit Gmail on Monday at 9:00"
5. Click action button to execute

### UI Design

- Clean card-based layout
- Color-coded icons per insight type
- Relevance score displayed
- One-click actions with loading states
- Empty state with call-to-action

## Technical Decisions & Rationale

### Why LLM-Based Segmentation?

**Alternative**: Fixed time gaps (e.g., 30min) + category matching

**Problem**: Arbitrary limits. User browsing Intercom docs for 2 hours = artificially split. Deep work sessions get fragmented.

**Solution**: LLM understands "exploring Intercom features" = one session.

### Why Not Pure ML?

**Problem**: Insufficient data per user (cold start), hard to interpret.

**Solution**: Hybrid approach - heuristics for fast pattern detection, LLM for intelligent interpretation.

### Why GPT-5-nano?

**GPT-5-nano**: Fast, cheap, perfect for all our tasks (session boundaries, theme naming, completion analysis). Cost-effective at $0.05/1M input tokens, $0.40/1M output tokens.

### Why Composite Scoring?

**Alternative**: Sort by frequency only

**Problem**: Recent but infrequent patterns get buried. Old but frequent patterns stay on top forever.

**Solution**: Balance frequency, recency, and impact. Ensures relevant insights surface.

## Defense for Interview Questions

### Q: Why not use just category matching for sessions?

**A**: Too coarse. "Technology" could be coding tutorial or shopping for laptop. Need full semantic understanding (descriptions, brands, subcategories).

### Q: How do you prevent LLM hallucination?

**A**: All analysis grounded in actual data (descriptions, categories, URLs, timestamps). Structured outputs, no generative tasks - only classification and summarization.

### Q: What if content analysis fails?

**A**: Graceful degradation. Falls back to time-gap segmentation. Pattern detection still works with partial data. System never breaks.

### Q: Why not local models (Ollama/Transformers.js)?

**A**: Quality matters for demo. Cost is acceptable ($5/year/user). Can optimize later with local models for specific tasks (session segmentation is a good candidate).

### Q: How does this scale?

**A**: Pattern detection runs offline (nightly batch or on-demand). Real-time only does matching. Async design with caching. No blocking operations.

### Q: What's the cold start problem?

**A**: Requires minimum 2 days of activity for meaningful patterns. Could add "Demo Mode" with synthetic data to showcase feature immediately.

## Future Enhancements

1. **Real-Time Suggestions**: Trigger insights based on current context (e.g., "You're on Gmail at your usual time, want to open Calendar too?")
2. **Workflow Automation**: One-click to create permanent shortcuts for detected workflows
3. **Insights Dashboard**: Visualize patterns over time (graphs, trends)
4. **Smart Reminders**: "You abandoned this research on pricing last week, want to continue?"
5. **Cross-User Patterns**: Anonymous aggregation to detect common workflows across users
6. **Local Models**: Replace GPT-5-nano with local embeddings + classifier for session segmentation

## Performance Characteristics

- **Analysis Time**: 10-30 seconds for 7 days of data (depends on session count)
- **Memory Usage**: ~50MB for pattern cache (7 days Ã— 50 activities/day)
- **Network**: Only outbound to OpenAI API (no data leaves user's machine otherwise)
- **Battery**: Minimal impact (analysis on-demand, not continuous)

## Demo Scenario

**Setup** (requires existing browsing data):
1. Browse some sites in patterns (e.g., news sites in morning, work tools mid-day)
2. Start a research session but don't complete it (search, read 3-4 pages, close)
3. Visit same domain at similar time multiple days

**Demo Flow**:
1. Open Blueberry Browser
2. Click "Insights" in sidebar
3. Click "Analyze Behavior"
4. Show detected patterns:
   - Workflow: "Morning news routine" â†’ Click "Open Workflow"
   - Research: "Researching X" â†’ Show key insights
   - Abandoned: "Incomplete research on Y" â†’ Click "Continue"
   - Habit: "You visit Z every morning"
5. Execute an action, show tabs opening

**Key Talking Points**:
- "This is proactive, not reactive"
- "Leverages all our existing data infrastructure"
- "LLM-powered semantic understanding, not just keyword matching"
- "Actionable insights, not just analytics"

## Conclusion

This feature demonstrates:
- âœ… Advanced AI/LLM skills (multi-model strategy, prompt engineering, cost optimization)
- âœ… System thinking (data flow, architecture, scaling)
- âœ… Product strategy (proactive > reactive)
- âœ… Full-stack capability (backend algorithms, IPC, React UI)
- âœ… Execution speed (implemented in <12h)

**This is the foundation for a truly intelligent browser.**

---

*Built in <12 hours. Ready to make Blueberry superior to Strawberry.* ðŸ«

